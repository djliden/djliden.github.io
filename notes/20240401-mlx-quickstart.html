<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>MLX Quickstart</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
<link rel="stylesheet" type="text/css" href="/orgstyle.css"/>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto+Slab&display=swap" rel="stylesheet">
<link rel="icon" href="favicon.ico" type="image/x-icon">
<link rel="me" href="https://emacs.ch/@dliden">
</head>
<body>
<div id="preamble" class="status">
<hr style="border-top: 1px solid black;">
<div class='topnav' style='display: flex; justify-content: space-between; align-items: center;'>
  <a href="/index.html"><h2 style='margin-top: 0; margin-bottom: 0; margin-left:0px;'>Daniel Liden</h2></a>
  <div>
    <a href='/archive.html' style='font-weight:bold; font-style:italic;'>Blog</a> / 
    <a href='/about.html' style='font-weight:bold; font-style:italic;'>About Me</a> /
    <a href='/photos.html' style='font-weight:bold; font-style:italic;'>Photos</a> /
    <a href='/notebooks/' style='font-weight:bold; font-style:italic;'>Notebooks</a> /
    <a href='/notes.html' style='font-weight:bold; font-style:italic;'>Notes</a> /
    <a href='/rss.xml'>
      <img src='/rss.png' style='height: 1em;'>
    </a>
  </div>
</div>
<hr style="border-top: 1px solid black;">
</div>
<div id="content" class="content">
<header>
<h1 class="title">MLX Quickstart</h1>
</header><nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org7166999">What is MLX?</a></li>
<li><a href="#org5a92c51">Quickstart Guide</a>
<ul>
<li><a href="#orgab84a0d">Basics</a></li>
<li><a href="#h:897CC44C-7D56-4606-B154-EED7083479BE">Lazy Evaluation in More Detail</a>
<ul>
<li><a href="#org25d5b37">When to evaluate</a></li>
</ul>
</li>
<li><a href="#orgcb9742d">Unified Memory</a></li>
<li><a href="#org916edad">Indexing arrays</a></li>
<li><a href="#org4e50f1b">Saving and Loading</a></li>
<li><a href="#org0e34cb4">Function transforms</a>
<ul>
<li><a href="#orgd6cc28a">Automatic Differentiation</a></li>
<li><a href="#orgabddc2c">Automatic Vectorization</a></li>
</ul>
</li>
<li><a href="#orgff90e91">Compilation</a></li>
<li><a href="#orgbdf116d">Streams</a></li>
</ul>
</li>
</ul>
</div>
</nav>
<div class="preview" id="org083e823">
<p>
These are my notes on the <a href="https://ml-explore.github.io/mlx/build/html/usage/quick_start.html">MLX quick start guide and usage notes</a>. It's a work in progress. Ultimately, I'm interested in learning what MLX will let me do with LLMs on my laptop. I might write something more substantial on that topic in the future. For now, you're probably better off consulting the docs yourself than looking at my notes on them.
</p>

</div>

<div id="outline-container-org7166999" class="outline-2">
<h2 id="org7166999">What is MLX?</h2>
<div class="outline-text-2" id="text-org7166999">
<p>
<a href="https://github.com/ml-explore/mlx">MLX</a> is an array framework from Apple ML Research. Its API follows NumPy. It has higher-level packages that follow PyTorch's API for building more complex models modularly. It features:
</p>
<ul class="org-ul">
<li>composable function transformations</li>
<li>lazy computation</li>
<li>dynamic computational graph construction</li>
<li><b>unified memory</b> model. Arrays live in shared memory, and operations on MLX arrays can be performed on CPU or GPU without the need to transfer data.</li>
</ul>
</div>
</div>
<div id="outline-container-org5a92c51" class="outline-2">
<h2 id="org5a92c51"><a href="https://ml-explore.github.io/mlx/build/html/usage/quick_start.html">Quickstart Guide</a></h2>
<div class="outline-text-2" id="text-org5a92c51">
<p>
First, install with <code>pip install mlx</code> or <code>conda install -c conda-forge mlx</code>.
</p>

<p>
Next, we'll work through the basic features, as shown in the quickstart guide linked above.
</p>
</div>

<div id="outline-container-orgab84a0d" class="outline-3">
<h3 id="orgab84a0d">Basics</h3>
<div class="outline-text-3" id="text-orgab84a0d">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> mlx.core <span class="org-keyword">as</span> mx

<span class="org-variable-name">a</span> <span class="org-operator">=</span> mx.array([1,2,3,4])
a.dtype
</pre>
</div>

<pre class="example">
mlx.core.int32
</pre>


<p>
Operations are lazy.
</p>


<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">b</span> <span class="org-operator">=</span> mx.array([1.0, 2.0, 3.0, 4.0])
<span class="org-variable-name">c</span> <span class="org-operator">=</span> a<span class="org-operator">+</span>b
</pre>
</div>

<p>
c is not computed until it is explicity called or until we use <code>eval</code>. See <a href="#h:897CC44C-7D56-4606-B154-EED7083479BE">Lazy Evaluation in More Detail</a>.
</p>

<div class="org-src-container">
<pre class="src src-python">mx.<span class="org-builtin">eval</span>(c)
c
</pre>
</div>

<pre class="example">
array([2, 4, 6, 8], dtype=float32)
</pre>


<p>
MLX has <code>grad</code> like PyTorch.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">x</span> <span class="org-operator">=</span> mx.array(0.0)
mx.sin(x)

mx.grad(mx.sin)(x)
</pre>
</div>

<pre class="example">
array(1, dtype=float32)
</pre>
</div>
</div>

<div id="outline-container-h:897CC44C-7D56-4606-B154-EED7083479BE" class="outline-3">
<h3 id="h:897CC44C-7D56-4606-B154-EED7083479BE">Lazy Evaluation in More Detail</h3>
<div class="outline-text-3" id="text-h:897CC44C-7D56-4606-B154-EED7083479BE">
<p>
When you perform an operation:
</p>
<ul class="org-ul">
<li>No computation happens</li>
<li>A compute graph is recorded</li>
<li>Computation happens once an <code>eval()</code> is performed.</li>
</ul>

<p>
PyTorch uses eager evaluation. Tensorflow uses lazy evaluation. Jax uses lazy eval. Jax and TF have different approaches to when they evaluate. TF/Jax graphs are compiled while MLX graphs are built dynamically.
</p>

<p>
One LLM-relevant use case: initializing model weights. You might initialize a model with <code>model = Model()</code>. The actual weight loading won't happen until you perform an <code>eval()</code>. Useful if you e.g. subsequently update the model with <code>float16</code> weights. You don't take the memory hit that you'd get with eager execution, loading the float32 weights.
</p>

<p>
It enables this pattern:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">model</span> <span class="org-operator">=</span> Model() <span class="org-comment-delimiter"># </span><span class="org-comment">no memory used yet</span>
model.load_weights(<span class="org-string">"weights_fp16.safetensors"</span>)
</pre>
</div>
</div>
<div id="outline-container-org25d5b37" class="outline-4">
<h4 id="org25d5b37">When to evaluate</h4>
<div class="outline-text-4" id="text-org25d5b37">
<p>
It's a tradeoff between:
</p>
<ul class="org-ul">
<li>letting graphs get too large</li>
<li>not batching enough to do useful work</li>
</ul>

<p>
There's a lot of flexibility.
</p>

<blockquote>
<p>
Luckily, a wide range of compute graph sizes work pretty well with MLX: anything from a few tens of operations to many thousands of operations per evaluation should be okay.
</p>
</blockquote>

<p>
Example of a good pattern for a training loop:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">for</span> batch <span class="org-keyword">in</span> dataset:

    <span class="org-comment-delimiter"># </span><span class="org-comment">Nothing has been evaluated yet</span>
    <span class="org-variable-name">loss</span>, <span class="org-variable-name">grad</span> <span class="org-operator">=</span> value_and_grad_fn(model, batch)

    <span class="org-comment-delimiter"># </span><span class="org-comment">Still nothing has been evaluated</span>
    optimizer.update(model, grad)

    <span class="org-comment-delimiter"># </span><span class="org-comment">Evaluate the loss and the new parameters which will</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">run the full gradient computation and optimizer update</span>
    mx.<span class="org-builtin">eval</span>(loss, model.parameters())
</pre>
</div>

<p>
Note: whenever you print an array or convert it to a numpy array, it is evaluated. Saving arrays will also evaluate them.
</p>

<p>
Using arrays for control flow will trigger an eval.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">def</span> <span class="org-function-name">fun</span>(x):
    <span class="org-variable-name">h</span>, <span class="org-variable-name">y</span> <span class="org-operator">=</span> first_layer(x)
    <span class="org-keyword">if</span> y <span class="org-operator">&gt;</span> 0:  <span class="org-comment-delimiter"># </span><span class="org-comment">An evaluation is done here!</span>
        <span class="org-variable-name">z</span>  <span class="org-operator">=</span> second_layer_a(h)
    <span class="org-keyword">else</span>:
        <span class="org-variable-name">z</span>  <span class="org-operator">=</span> second_layer_b(h)
    <span class="org-keyword">return</span> z
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgcb9742d" class="outline-3">
<h3 id="orgcb9742d">Unified Memory</h3>
<div class="outline-text-3" id="text-orgcb9742d">
<p>
You do not need to specify the location of an MLX array in memory. CPU and GPU share memory.
</p>

<p>
Instead of moving arrays to devices, you specify the device when you run an operation.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">a</span> <span class="org-operator">=</span> mx.random.normal((100,))
<span class="org-variable-name">b</span> <span class="org-operator">=</span> mx.random.normal((100,))
</pre>
</div>

<pre class="example">
None
</pre>



<div class="org-src-container">
<pre class="src src-python">mx.add(a, b, stream<span class="org-operator">=</span>mx.cpu)

</pre>
</div>

<pre class="example">
array([-0.999945, -0.255963, 1.04271, ..., 1.08311, -0.993303, -1.48334], dtype=float32)
</pre>


<div class="org-src-container">
<pre class="src src-python">mx.add(a, b, stream<span class="org-operator">=</span>mx.gpu)
</pre>
</div>

<pre class="example">
array([-0.999945, -0.255963, 1.04271, ..., 1.08311, -0.993303, -1.48334], dtype=float32)
</pre>


<p>
The MLX scheduler will manage dependencies to avoid race conditions. In other words, this is fine.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">c</span> <span class="org-operator">=</span> mx.add(a, b, stream<span class="org-operator">=</span>mx.cpu)
<span class="org-variable-name">d</span> <span class="org-operator">=</span> mx.add(a, c, stream<span class="org-operator">=</span>mx.gpu)
</pre>
</div>

<pre class="example">
None
</pre>


<p>
This can be useful if we e.g. send compute-dense operatios to GPU, smaller overhead-bound operations to cpu like <a href="https://ml-explore.github.io/mlx/build/html/usage/unified_memory.html#a-simple-example">this example</a>.
</p>
</div>
</div>
<div id="outline-container-org916edad" class="outline-3">
<h3 id="org916edad">Indexing arrays</h3>
<div class="outline-text-3" id="text-org916edad">
<ul class="org-ul">
<li>Is the same as NumPy in most cases</li>
<li>EXCEPT:
<ul class="org-ul">
<li>It does not perform bounds checking. Indexing out of bounds is undefined behavior. Why? Exceptions can't propagate from the GPU.</li>
<li>Boolean mask indexing is not supported (yet).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org4e50f1b" class="outline-3">
<h3 id="org4e50f1b">Saving and Loading</h3>
<div class="outline-text-3" id="text-org4e50f1b">
<p>
Support for numpy, numpy archive, safetensors, gguf.
</p>
</div>
</div>

<div id="outline-container-org0e34cb4" class="outline-3">
<h3 id="org0e34cb4">Function transforms</h3>
<div class="outline-text-3" id="text-org0e34cb4">
<p>
MLX uses <i>composable function transformations</i> for autodiff, vectorization, graph optimization. Main idea: every transformation returns a function that can be further transformed. Here is an example.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">dfdx</span> <span class="org-operator">=</span> mx.grad(mx.sin)
dfdx(mx.array(mx.pi))

</pre>
</div>

<pre class="example">
array(-1, dtype=float32)
</pre>


<p>
The output of <code>grad</code> on <code>sin</code> is another function: the gradient of the sine function. To get the second derivative, just do <code>mx.grad(mx.grad())</code>. You can compose any function transform in any order to any depth.
</p>
</div>
<div id="outline-container-orgd6cc28a" class="outline-4">
<h4 id="orgd6cc28a">Automatic Differentiation</h4>
<div class="outline-text-4" id="text-orgd6cc28a">
<p>
Autodiff works on functions, not on implicit graphs. <i>This is a key difference from PyTorch</i>. In PyTorch, autodiff works on implicit graphs.
</p>

<p>
By default, the gradient is computed w/r/t the first argument. But we can specify the argument.
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">def</span> <span class="org-function-name">loss_fn</span>(w, x, y):
   <span class="org-keyword">return</span> mx.mean(mx.square(w <span class="org-operator">*</span> x <span class="org-operator">-</span> y))

<span class="org-variable-name">w</span> <span class="org-operator">=</span> mx.array(1.0)
<span class="org-variable-name">x</span> <span class="org-operator">=</span> mx.array([0.5, <span class="org-operator">-</span>0.5])
<span class="org-variable-name">y</span> <span class="org-operator">=</span> mx.array([1.5, <span class="org-operator">-</span>1.5])

<span class="org-comment-delimiter"># </span><span class="org-comment">Computes the gradient of loss_fn with respect to w:</span>
<span class="org-variable-name">grad_fn</span> <span class="org-operator">=</span> mx.grad(loss_fn)
<span class="org-variable-name">dloss_dw</span> <span class="org-operator">=</span> grad_fn(w, x, y)
<span class="org-comment-delimiter"># </span><span class="org-comment">Prints array(-1, dtype=float32)</span>
dloss_dw
</pre>
</div>

<pre class="example">
array(-1, dtype=float32)
</pre>


<div class="org-src-container">
<pre class="src src-python"><span class="org-comment-delimiter"># </span><span class="org-comment">To get the gradient with respect to x we can do:</span>
<span class="org-variable-name">grad_fn</span> <span class="org-operator">=</span> mx.grad(loss_fn, argnums<span class="org-operator">=</span>1)
<span class="org-variable-name">dloss_dx</span> <span class="org-operator">=</span> grad_fn(w, x, y)
<span class="org-comment-delimiter"># </span><span class="org-comment">Prints array([-1, 1], dtype=float32)</span>
dloss_dx
</pre>
</div>

<pre class="example">
array([-1, 1], dtype=float32)
</pre>


<p>
The <code>value_and_grad</code> function provides an efficient way to get the value and the gradient e.g. of the loss.
</p>


<div class="org-src-container">
<pre class="src src-python"><span class="org-comment-delimiter"># </span><span class="org-comment">Computes the gradient of loss_fn with respect to w:</span>
<span class="org-variable-name">loss_and_grad_fn</span> <span class="org-operator">=</span> mx.value_and_grad(loss_fn)
<span class="org-variable-name">loss</span>, <span class="org-variable-name">dloss_dw</span> <span class="org-operator">=</span> loss_and_grad_fn(w, x, y)

<span class="org-comment-delimiter"># </span><span class="org-comment">Prints array(1, dtype=float32)</span>
<span class="org-builtin">print</span>(loss)

<span class="org-comment-delimiter"># </span><span class="org-comment">Prints array(-1, dtype=float32)</span>
<span class="org-builtin">print</span>(dloss_dw)
</pre>
</div>

<pre class="example">
array(1, dtype=float32)
array(-1, dtype=float32)
</pre>


<p>
You can use <code>stop_gradient()</code> to stop gradients from propagating through a part of the function.
</p>
</div>
</div>
<div id="outline-container-orgabddc2c" class="outline-4">
<h4 id="orgabddc2c">Automatic Vectorization</h4>
<div class="outline-text-4" id="text-orgabddc2c">
<p>
<code>vmap()</code> automatically vectorizes complex functions.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-comment-delimiter"># </span><span class="org-comment">Vectorize over the second dimension of x and the</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">first dimension of y</span>
<span class="org-variable-name">vmap_add</span> <span class="org-operator">=</span> mx.vmap(<span class="org-keyword">lambda</span> x, y: x <span class="org-operator">+</span> y, in_axes<span class="org-operator">=</span>(1, 0))
</pre>
</div>

<pre class="example">
None
</pre>


<p>
<code>in_axes</code> specifies which dimensions of the input to vectorize over. <code>out_axes</code> specifies where they should be in the output.
</p>
</div>
</div>
</div>
<div id="outline-container-orgff90e91" class="outline-3">
<h3 id="orgff90e91">Compilation</h3>
<div class="outline-text-3" id="text-orgff90e91">
<p>
MLX has a <code>compile</code> function for compiling computational graphs. What does compilation mean in this context? Compilation makes <i>smaller graphs</i> by merging common work and fusing common operations.
</p>

<p>
The first time you call a compiled function, MLX builds and optimizes the compute graph and generates and compiles the code. This can be slow, but the resulting compiled function is cached, so subsequent calls do not initiate a new compilation.
</p>

<p>
What causes a function to be recompiled?
</p>
<ul class="org-ul">
<li>changing shape or number of dimensions</li>
<li>changing the type of any inputs</li>
<li>changing the number of inputs</li>
</ul>

<p>
Don't compile functions that are created and destroyed frequently.
</p>

<p>
Debugging can be tricky. When a compiled function is first called, it is traced with placeholder inputs, so it will crash if there's a print statement. For debugging purposes, disable compilation with <code>disable_compile</code> or setting the <code>MLX_DISABLE_COMPILE</code> flag.
</p>

<p>
Compiled functions should be pure. They should not have side effects. Review this section for functions that update some saved state.
</p>
</div>
</div>
<div id="outline-container-orgbdf116d" class="outline-3">
<h3 id="orgbdf116d">Streams</h3>
<div class="outline-text-3" id="text-orgbdf116d">
<p>
All operations take an optional <code>stream</code> keyword specifying which Stream the operation should run on. This is for specifying the device to run on.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2024-04-01 Mon 00:00</p>
<p class="creator"><a href="https://www.gnu.org/software/emacs/">Emacs</a> 29.3 (<a href="https://orgmode.org">Org</a> mode 9.6.15)</p>
</div>
</body>
</html>
