<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Troubleshooting Flash Attention Installation</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
<link rel="stylesheet" type="text/css" href="/orgstyle.css"/>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto+Slab&display=swap" rel="stylesheet">
<link rel="icon" href="favicon.ico" type="image/x-icon">
<link rel="me" href="https://emacs.ch/@dliden">
</head>
<body>
<div id="preamble" class="status">
<hr style="border-top: 1px solid black;">
<div class='topnav' style='display: flex; justify-content: space-between; align-items: center;'>
  <a href="/index.html"><h2 style='margin-top: 0; margin-bottom: 0; margin-left:0px;'>Daniel Liden</h2></a>
  <div>
    <a href='/archive.html' style='font-weight:bold; font-style:italic;'>Blog</a> / 
    <a href='/about.html' style='font-weight:bold; font-style:italic;'>About Me</a> /
    <a href='/photos.html' style='font-weight:bold; font-style:italic;'>Photos</a> /
    <a href='/notebooks/' style='font-weight:bold; font-style:italic;'>Notebooks</a> /
    <a href='/notes.html' style='font-weight:bold; font-style:italic;'>Notes</a> /
    <a href='/rss.xml'>
      <img src='/rss.png' style='height: 1em;'>
    </a>
  </div>
</div>
<hr style="border-top: 1px solid black;">
</div>
<div id="content" class="content">
<header>
<h1 class="title">Troubleshooting Flash Attention Installation</h1>
</header><nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org0f5e989">The Problem</a></li>
<li><a href="#org00db43c">Solution</a></li>
<li><a href="#orgd265980">What didn't work</a></li>
</ul>
</div>
</nav>
<div class="preview" id="org0c621b1">
<p>
I have repeatedly run into issues getting flash-attention working correctly with whatever version of PyTorch and CUDA I happen to be working with. I found a working pattern, at least for the platform I tend to be working on (Databricks). This note is a quick summary.
</p>

</div>
<div id="outline-container-org0f5e989" class="outline-2">
<h2 id="org0f5e989">The Problem</h2>
<div class="outline-text-2" id="text-org0f5e989">
<p>
I kept getting an "undefined symbol" error like <a href="https://github.com/Dao-AILab/flash-attention/issues/667">this</a> when trying to load a model with flash attention (or even just when importing the flash attention library).
</p>
</div>
</div>

<div id="outline-container-org00db43c" class="outline-2">
<h2 id="org00db43c">Solution</h2>
<div class="outline-text-2" id="text-org00db43c">
<p>
The following approach worked.
</p>

<ol class="org-ol">
<li>Verify CUDA version; install the right version of Torch.</li>
<li>Clone the flash-attention library and install (don't just pip install)</li>
</ol>

<p>
So in the case of my most recent project:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-operator">%</span>pip install <span class="org-operator">--</span>upgrade torch
</pre>
</div>

<p>
was fine because it's compiled for cuda 12.
</p>

<p>
To install <code>flash-attention</code>:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-operator">%</span>sh
git clone https:<span class="org-operator">//</span>github.com<span class="org-operator">/</span>Dao<span class="org-operator">-</span>AILab<span class="org-operator">/</span>flash<span class="org-operator">-</span>attention.git
cd flash<span class="org-operator">-</span>attention
pip install . <span class="org-operator">--</span>no<span class="org-operator">-</span>build<span class="org-operator">-</span>isolation
</pre>
</div>

<p>
We can then make sure everything works (without needing to take extra time to load a model, for example) like this:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> torch
<span class="org-builtin">print</span>(torch.__version__)
<span class="org-builtin">print</span>(torch.version.cuda)

<span class="org-keyword">import</span> flash_attn
<span class="org-builtin">print</span>(flash_attn.__version__)
</pre>
</div>

<pre class="example">
2.2.2+cu121
12.1
2.5.7
</pre>
</div>
</div>

<div id="outline-container-orgd265980" class="outline-2">
<h2 id="orgd265980">What didn't work</h2>
<div class="outline-text-2" id="text-orgd265980">
<p>
I wasn't able to get any variety of <code>pip install flash-attn</code> working. This was regardless of the no build isolation flag; specific versions; etc.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2024-04-16 Tue 00:00</p>
<p class="creator"><a href="https://www.gnu.org/software/emacs/">Emacs</a> 29.3 (<a href="https://orgmode.org">Org</a> mode 9.6.15)</p>
</div>
</body>
</html>
