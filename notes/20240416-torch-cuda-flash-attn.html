<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Troubleshooting Flash Attention Installation</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="/org-base.css"/>
<link rel="stylesheet" type="text/css" href="/orgstyle.css"/>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@400;500;600;700&display=swap" rel="stylesheet">
<link rel="icon" href="/favicon.ico" type="image/x-icon">
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
<link rel="me" href="https://emacs.ch/@dliden">
<link rel="alternate" type="application/rss+xml" title="Daniel Liden's Blog" href="/rss.xml">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<script src="/dark-mode.js" defer></script>
<script src="/admonitions.js" defer></script>
<link rel="canonical" href="https://danliden.com/notes/20240416-torch-cuda-flash-attn.html">
<meta name="author" content="Daniel Liden">
<meta property="og:url" content="https://danliden.com/notes/20240416-torch-cuda-flash-attn.html">
<meta property="og:title" content="Troubleshooting Flash Attention Installation">
<meta property="og:description" content="I have repeatedly run into issues getting flash-attention working correctly with whatever version of PyTorch and CUDA I happen to be working with. I found a working pattern, at least for the platfo...">
<meta property="og:site_name" content="Daniel Liden">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2024-04-16T00:00:00Z">
<meta property="article:modified_time" content="2025-10-02T15:00:34Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Troubleshooting Flash Attention Installation">
<meta name="twitter:description" content="I have repeatedly run into issues getting flash-attention working correctly with whatever version of PyTorch and CUDA I happen to be working with. I found a working pattern, at least for the platfo...">
<script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Troubleshooting Flash Attention Installation","description":"I have repeatedly run into issues getting flash-attention working correctly with whatever version of PyTorch and CUDA I happen to be working with. I found a working pattern, at least for the platfo...","mainEntityOfPage":{"@type":"WebPage","@id":"https://danliden.com/notes/20240416-torch-cuda-flash-attn.html"},"author":{"@type":"Person","name":"Daniel Liden"},"datePublished":"2024-04-16T00:00:00Z","dateModified":"2025-10-02T15:00:34Z"}</script>
</head>
<body>
<div id="preamble" class="status">
<hr class="topnav-rule">
<header class="site-header">
  <a href="/index.html" class="site-header__brand">
    <h2>Daniel Liden</h2>
  </a>
  <nav class="topnav-links" aria-label="Primary">
    <a href="/archive.html">Blog</a>
    <span class="topnav-divider" aria-hidden="true">/</span>
    <a href="/about.html">About Me</a>
    <span class="topnav-divider" aria-hidden="true">/</span>
    <a href="/photos.html">Photos</a>
    <span class="topnav-divider" aria-hidden="true">/</span>
    <a href="/notebooks/">Notebooks</a>
    <span class="topnav-divider" aria-hidden="true">/</span>
    <a href="/notes.html">Notes</a>
    <span class="topnav-divider" aria-hidden="true">/</span>
    <a class="topnav-icon" href="/rss.xml" aria-label="RSS feed">
      <svg class="rss-icon" viewBox="0 0 24 24" aria-hidden="true" fill="none" stroke="currentColor" stroke-width="1.8" stroke-linecap="round" stroke-linejoin="round">
        <path d="M5 4h1a14 14 0 0 1 14 14v1"/>
        <path d="M5 11h1a7 7 0 0 1 7 7v1"/>
        <circle cx="6" cy="18" r="2"/>
      </svg>
    </a>
    <span class="topnav-divider" aria-hidden="true">/</span>
    <button id="theme-toggle" class="nav-theme-toggle topnav-icon" type="button" aria-label="Switch to dark mode" aria-pressed="false">
      <svg class="theme-icon" viewBox="0 0 24 24" aria-hidden="true" fill="currentColor">
        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79Z"/>
      </svg>
    </button>
  </nav>
</header>
<hr class="topnav-rule">
</div>
<div id="content" class="content">
<header>
<h1 class="title">Troubleshooting Flash Attention Installation</h1>
</header><nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org226a76f">The Problem</a></li>
<li><a href="#orgf012820">Solution</a></li>
<li><a href="#orgcde6bd7">What didn't work</a></li>
</ul>
</div>
</nav>
<div class="preview" id="org048cfaf">
<p>
I have repeatedly run into issues getting flash-attention working correctly with whatever version of PyTorch and CUDA I happen to be working with. I found a working pattern, at least for the platform I tend to be working on (Databricks). This note is a quick summary.
</p>

</div>
<div id="outline-container-org226a76f" class="outline-2">
<h2 id="org226a76f">The Problem</h2>
<div class="outline-text-2" id="text-org226a76f">
<p>
I kept getting an "undefined symbol" error like <a href="https://github.com/Dao-AILab/flash-attention/issues/667">this</a> when trying to load a model with flash attention (or even just when importing the flash attention library).
</p>
</div>
</div>

<div id="outline-container-orgf012820" class="outline-2">
<h2 id="orgf012820">Solution</h2>
<div class="outline-text-2" id="text-orgf012820">
<p>
The following approach worked.
</p>

<ol class="org-ol">
<li>Verify CUDA version; install the right version of Torch.</li>
<li>Clone the flash-attention library and install (don't just pip install)</li>
</ol>

<p>
So in the case of my most recent project:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-operator">%</span>pip install <span class="org-operator">--</span>upgrade torch
</pre>
</div>

<p>
was fine because it's compiled for cuda 12.
</p>

<p>
To install <code>flash-attention</code>:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-operator">%</span>sh
git clone https:<span class="org-operator">//</span>github.com<span class="org-operator">/</span>Dao<span class="org-operator">-</span>AILab<span class="org-operator">/</span>flash<span class="org-operator">-</span>attention.git
cd flash<span class="org-operator">-</span>attention
pip install . <span class="org-operator">--</span>no<span class="org-operator">-</span>build<span class="org-operator">-</span>isolation
</pre>
</div>

<p>
We can then make sure everything works (without needing to take extra time to load a model, for example) like this:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> torch
<span class="org-builtin">print</span>(torch.__version__)
<span class="org-builtin">print</span>(torch.version.cuda)

<span class="org-keyword">import</span> flash_attn
<span class="org-builtin">print</span>(flash_attn.__version__)
</pre>
</div>

<pre class="example">
2.2.2+cu121
12.1
2.5.7
</pre>
</div>
</div>

<div id="outline-container-orgcde6bd7" class="outline-2">
<h2 id="orgcde6bd7">What didn't work</h2>
<div class="outline-text-2" id="text-orgcde6bd7">
<p>
I wasn't able to get any variety of <code>pip install flash-attn</code> working. This was regardless of the no build isolation flag; specific versions; etc.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2024-04-16 Tue 00:00</p>
<p class="creator"><a href="https://www.gnu.org/software/emacs/">Emacs</a> 29.3 (<a href="https://orgmode.org">Org</a> mode 9.6.15)</p>
</div>
</body>
</html>
