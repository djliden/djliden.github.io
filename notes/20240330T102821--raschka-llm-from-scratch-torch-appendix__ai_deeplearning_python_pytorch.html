<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>PyTorch Review</title>
<meta name="generator" content="Org mode">
<link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
<link rel="stylesheet" type="text/css" href="/orgstyle.css"/>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto+Slab&display=swap" rel="stylesheet">
<link rel="icon" href="favicon.ico" type="image/x-icon">
<link rel="me" href="https://emacs.ch/@dliden">
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2020 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="preamble" class="status">
<hr style="border-top: 1px solid black;">
<div class='topnav' style='display: flex; justify-content: space-between; align-items: center;'>
  <a href="/index.html"><h2 style='margin-top: 0; margin-bottom: 0; margin-left:0px;'>Daniel Liden</h2></a>
  <div>
    <a href='/archive.html' style='font-weight:bold; font-style:italic;'>Blog</a> / 
    <a href='/about.html' style='font-weight:bold; font-style:italic;'>About Me</a> /
    <a href='/photos.html' style='font-weight:bold; font-style:italic;'>Photos</a> /
    <a href='/fine-tuning/' style='font-weight:bold; font-style:italic;'>LLM Fine Tuning</a> /
    <a href='/notes.html' style='font-weight:bold; font-style:italic;'>Notes</a> /
    <a href='/rss.xml'>
      <img src='/rss.png' style='height: 1em;'>
    </a>
  </div>
</div>
<hr style="border-top: 1px solid black;">
</div>
<div id="content">
<header>
<h1 class="title">PyTorch Review</h1>
</header><nav id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org60a222d">A.1 PyTorch</a></li>
<li><a href="#org2c6310c">A.2 Understanding tensors</a>
<ul>
<li><a href="#org4fa11db">Creating tensors</a></li>
<li><a href="#org7e94ba8">Tensor data types</a></li>
<li><a href="#org885ed33">Common tensor operations</a></li>
</ul>
</li>
<li><a href="#orgd1aab9a">A.3 Models as Computational Graphs</a></li>
<li><a href="#orge4db986">A.4 Automatic Differentiation</a></li>
<li><a href="#orgb2f8f12">A.5 Implementing multilayer neural networks</a></li>
<li><a href="#org5ff72ee">A.6 Setting up efficient data loaders</a></li>
</ul>
</div>
</nav>
<div class="preview">
<p>
This is a quick run through the appendix on PyTorch from Sebastian Raschka's <a href="https://www.manning.com/books/build-a-large-language-model-from-scratch">Build a Large Language Model (From Scratch)</a> book, currently available via Manning's MEAP. I haven't spent an enormous amount of time with PyTorch in the last year or so, so it seemed worth the effort to work through it.
</p>

</div>

<div id="outline-container-org60a222d" class="outline-2">
<h2 id="org60a222d">A.1 PyTorch</h2>
<div class="outline-text-2" id="text-org60a222d">
<p>
There are three broad components to PyTorch:
</p>
<ul class="org-ul">
<li>A tensor library extending array-oriented programming from NumPy with additional features for accelerated computation on GPUs.</li>
<li>An automatic differentiation engine (autograd), which ehables automatic computation of gradients for tensor operations for backpropagation/model optimization.</li>
<li>A deep learning library, offering modular, flexible, and extensible building blocks for designing and training deep learning models.</li>
</ul>

<p>
Let's make sure we have it installed correctly&#x2026;
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> torch

torch.__version__
</pre>
</div>

<pre class="example">
2.2.2
</pre>


<p>
Let's make sure we can use <code>mps</code> (on mac).
</p>

<div class="org-src-container">
<pre class="src src-python">torch.backends.mps.is_available()
</pre>
</div>

<pre class="example">
True
</pre>


<p>
Great.
</p>
</div>
</div>
<div id="outline-container-org2c6310c" class="outline-2">
<h2 id="org2c6310c">A.2 Understanding tensors</h2>
<div class="outline-text-2" id="text-org2c6310c">
<p>
Tensors generalize vectors and matrices to arbitrary dimensions. PyTorch tensors are similar to NumPy arrays but have have several additional features:
</p>
<ul class="org-ul">
<li>an automatic differentiation engine</li>
<li>gpu computation</li>
</ul>
<p>
Still, it has a numpy-like API.
</p>
</div>
<div id="outline-container-org4fa11db" class="outline-3">
<h3 id="org4fa11db">Creating tensors</h3>
<div class="outline-text-3" id="text-org4fa11db">
<div class="org-src-container">
<pre class="src src-python"><span class="org-comment-delimiter"># </span><span class="org-comment">0d tensor (scalar)</span>
<span class="org-keyword">print</span>(torch.tensor(1))

<span class="org-comment-delimiter"># </span><span class="org-comment">1d tensor (vector)</span>
<span class="org-keyword">print</span>(torch.tensor([1, 2, 3]))

<span class="org-comment-delimiter"># </span><span class="org-comment">2d tensor</span>
<span class="org-keyword">print</span>(torch.tensor([[1, 2], [3, 4]]))

<span class="org-comment-delimiter"># </span><span class="org-comment">3d tensor</span>
<span class="org-keyword">print</span>(torch.tensor([[[1, 2], [3,4]], [[5,6], [7,8]]]))
</pre>
</div>

<pre class="example">
tensor(1)
tensor([1, 2, 3])
tensor([[1, 2],
        [3, 4]])
tensor([[[1, 2],
         [3, 4]],

        [[5, 6],
         [7, 8]]])
</pre>
</div>
</div>
<div id="outline-container-org7e94ba8" class="outline-3">
<h3 id="org7e94ba8">Tensor data types</h3>
<div class="outline-text-3" id="text-org7e94ba8">
<p>
These are important to pay attention to! So let's pay attention to them. The default (from above) is the 64-bit integer.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">tensor1d</span> = torch.tensor([1,2,3])
<span class="org-keyword">print</span>(tensor1d.dtype)
</pre>
</div>

<pre class="example">
torch.int64
</pre>


<p>
For floats, PyTorch uses 32-bit precision by default.
</p>


<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">floatvec</span> = torch.tensor([1., 2., 3.])
<span class="org-keyword">print</span>(floatvec.dtype)
</pre>
</div>

<pre class="example">
torch.float32
</pre>


<p>
Why this default?
</p>
<ul class="org-ul">
<li>GPU architectures are optimized for 32-bit computations</li>
<li>32-bit precision is sufficient for most deep learning tasks but uses less memory and computational resources than 64-bit.</li>
</ul>

<p>
it is easy to change <code>dtype</code> (and precision) with a tensor's <code>.to</code> method.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(torch.tensor([1,2,3]).to(torch.float32).dtype)
</pre>
</div>

<pre class="example">
torch.float32
</pre>
</div>
</div>
<div id="outline-container-org885ed33" class="outline-3">
<h3 id="org885ed33">Common tensor operations</h3>
<div class="outline-text-3" id="text-org885ed33">
<ul class="org-ul">
<li>brief survey of the most common tensor operations prior to getting into the computational graphs concept.</li>
</ul>


<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">tensor2d</span> = torch.tensor([[1, 2, 3], [4, 5, 6]])
</pre>
</div>

<p>
Reshape:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(tensor2d.reshape(3, 2))
</pre>
</div>

<pre class="example">
tensor([[1, 2],
        [3, 4],
        [5, 6]])
</pre>


<p>
It is more common to use <code>view</code> than <code>reshape</code>.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(tensor2d.view(3, 2))
</pre>
</div>

<pre class="example">
tensor([[1, 2],
        [3, 4],
        [5, 6]])
</pre>


<p>
Transpose
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(tensor2d.T)
</pre>
</div>

<pre class="example">
tensor([[1, 4],
        [2, 5],
        [3, 6]])
</pre>



<p>
Matrix multiplication is usually handled with <code>matmul</code>.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(tensor2d.matmul(tensor2d.T))
</pre>
</div>

<pre class="example">
tensor([[14, 32],
        [32, 77]])
</pre>


<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(tensor2d @ tensor2d.T)
</pre>
</div>

<pre class="example">
tensor([[14, 32],
        [32, 77]])
</pre>
</div>
</div>
</div>
<div id="outline-container-orgd1aab9a" class="outline-2">
<h2 id="orgd1aab9a">A.3 Models as Computational Graphs</h2>
<div class="outline-text-2" id="text-orgd1aab9a">
<p>
The previous section covered PyTorch's tensor library. This section gets into its automatic differentiation engine (autograd). Autograd provides functions for automatically computing gradients in dynamic computational graphs.
</p>

<p>
So what's a computational graph? It lays out the sequence of calculations needed to compute the gradients for backprop. We'll go through an example showing the forward pass of a logstic regression classifier.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> torch.nn.functional <span class="org-keyword">as</span> F

<span class="org-variable-name">y</span> = torch.tensor([1.0])
<span class="org-variable-name">x1</span> = torch.tensor([1.1])
<span class="org-variable-name">w1</span> = torch.tensor([2.2])
<span class="org-variable-name">b</span> = torch.tensor([0.0])

<span class="org-variable-name">z</span> = x1 * w1 + b
<span class="org-variable-name">a</span> = torch.sigmoid(z)

<span class="org-variable-name">loss</span> = F.binary_cross_entropy(a,y)
</pre>
</div>

<p>
This results in a computational graph which PyTorch builds in the background.
</p>

<p>
Input and weight -&gt; (u = w<sub>1</sub> * x<sub>1</sub>) -&gt; +b -&gt; (z = u + b) -&gt; (a = &sigma;(z)) -&gt; loss = L(a,y) &lt;- y
</p>
</div>
</div>
<div id="outline-container-orge4db986" class="outline-2">
<h2 id="orge4db986">A.4 Automatic Differentiation</h2>
<div class="outline-text-2" id="text-orge4db986">
<p>
PyTorch will automatically build such a graph if one of its terminal nodes has the <code>requires_grad</code> attribute set to True. This enables us to train neural nets via backpropagation. Working backward from the above:
</p>

\begin{align*}
\frac{\partial L}{\partial w_1} &= \frac{\partial u}{\partial w_1} \times \frac{\partial z}{\partial u} \times \frac{\partial a}{\partial z} \times \frac{\partial L}{\partial a} \\
\frac{\partial L}{\partial b} &= \frac{\partial z}{\partial b} \times \frac{\partial a}{\partial z} \times \frac{\partial L}{\partial a}
\end{align*}

<p>
Basically&#x2013;apply the chain rule right to left.
</p>

<p>
Quick reminder of some definitions:
</p>
<ul class="org-ul">
<li>a partial derivative measures the rate at which a function changes w/r/t one of its variables</li>
<li>a gradient is a vector of all the partial derivatives of a multivariate function</li>
</ul>

<p>
So what exactly does this have to do with torch as an autograd engine? PyTorch tracks every operation performed on tensors and can, therefore, construct a computational graph in the background. Then it cal cann on the <code>grad</code> function to compute the gradient of the loss w/r/t the model parameter as follows:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> torch.nn.functional <span class="org-keyword">as</span> F
<span class="org-keyword">from</span> torch.autograd <span class="org-keyword">import</span> grad

<span class="org-variable-name">y</span> = torch.tensor([1.0])
<span class="org-variable-name">x1</span> = torch.tensor([1.1])
<span class="org-variable-name">w1</span> = torch.tensor([2.2], requires_grad=<span class="org-constant">True</span>)
<span class="org-variable-name">b</span> = torch.tensor([0.0], requires_grad=<span class="org-constant">True</span>)

<span class="org-variable-name">z</span> = x1 * w1 + b
<span class="org-variable-name">a</span> = torch.sigmoid(z)

<span class="org-variable-name">loss</span> = F.binary_cross_entropy(a, y)
<span class="org-variable-name">grad_L_w1</span> = grad(loss, w1, retain_graph=<span class="org-constant">True</span>) <span class="org-comment-delimiter">#</span><span class="org-comment">A</span>
<span class="org-variable-name">grad_L_b</span> = grad(loss, b, retain_graph=<span class="org-constant">True</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(grad_L_w1)
<span class="org-keyword">print</span>(grad_L_b)
</pre>
</div>

<pre class="example">
(tensor([-0.0898]),)
(tensor([-0.0817]),)
</pre>


<p>
We seldom manually call the grad function. We usually call <code>.backward</code> on the loss, which computes the gradients of all the leaf nodes in the graph, which will be stored via the <code>.grad</code> attributes of the tensors.
</p>


<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(loss.backward())
<span class="org-keyword">print</span>(w1.grad)
<span class="org-keyword">print</span>(b.grad)
</pre>
</div>

<pre class="example">
None
tensor([-0.0898])
tensor([-0.0817])
</pre>
</div>
</div>
<div id="outline-container-orgb2f8f12" class="outline-2">
<h2 id="orgb2f8f12">A.5 Implementing multilayer neural networks</h2>
<div class="outline-text-2" id="text-orgb2f8f12">
<p>
Now we get to the third major component of Pytorch: its library for implementing deep neural networks.
</p>

<p>
We will focus on a fully-connected MLP. To implement an NN in PyTorch, we:
</p>
<ul class="org-ul">
<li>subclass the <code>torch.nn.Module</code> class to define a custom architecture</li>
<li>define layers within the <code>__init__</code> constructor of the module subclass, specifying how they interact in the forward method.</li>
<li>defined the forward method, which describes how data passes through the network and relates as a computational graph.</li>
</ul>

<p>
We generally do not need to implement the <code>backward</code> method ourselves.
</p>

<p>
Here is code illustrating a basic NN with two hidden layers.
</p>


<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">class</span> <span class="org-type">NeuralNetwork</span>(torch.nn.Module):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, num_inputs, num_outputs):
        <span class="org-builtin">super</span>().__init__()

        <span class="org-keyword">self</span>.layers = torch.nn.Sequential(
            <span class="org-comment-delimiter"># </span><span class="org-comment">1st hidden layer</span>
            torch.nn.Linear(num_inputs, 30),
            torch.nn.ReLU(),
            <span class="org-comment-delimiter"># </span><span class="org-comment">2nd hidden layer</span>
            torch.nn.Linear(30, 20),
            torch.nn.ReLU(),
            <span class="org-comment-delimiter"># </span><span class="org-comment">output layer</span>
            torch.nn.Linear(20, num_outputs),
        )

    <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x):
        <span class="org-variable-name">logits</span> = <span class="org-keyword">self</span>.layers(x)
        <span class="org-keyword">return</span> logits
</pre>
</div>

<p>
We can instantiate this with 50 inputs and 3 outputs.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">model</span> = NeuralNetwork(50, 3)
<span class="org-keyword">print</span>(model)
</pre>
</div>

<pre class="example">
NeuralNetwork(
  (layers): Sequential(
    (0): Linear(in_features=50, out_features=30, bias=True)
    (1): ReLU()
    (2): Linear(in_features=30, out_features=20, bias=True)
    (3): ReLU()
    (4): Linear(in_features=20, out_features=3, bias=True)
  )
)
</pre>


<p>
We can count the total number of trainable parameters as follows:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">num_params</span> = <span class="org-builtin">sum</span>(p.numel() <span class="org-keyword">for</span> p <span class="org-keyword">in</span> model.parameters() <span class="org-keyword">if</span> p.requires_grad)
<span class="org-keyword">print</span>(<span class="org-string">"Total number of trainable model parameters:"</span>, num_params)
</pre>
</div>

<pre class="example">
Total number of trainable model parameters: 2213
</pre>


<p>
A parameter is <i>trainable</i> if its <code>requires_grad</code> attribute is <code>True</code>. We can investigate specific layers. Let's look at the first linear layer.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(model.layers[0].weight)
</pre>
</div>

<pre class="example">
Parameter containing:
tensor([[-0.0844,  0.0863,  0.1168,  ...,  0.0203, -0.0814, -0.0504],
        [ 0.0288,  0.0004, -0.1411,  ..., -0.0322, -0.1085,  0.0682],
        [-0.1075, -0.0173, -0.0476,  ..., -0.0684, -0.0522, -0.1316],
        ...,
        [ 0.1129, -0.0639, -0.0662,  ...,  0.1284, -0.0707,  0.1090],
        [ 0.0790, -0.1206, -0.1156,  ...,  0.1393, -0.0233,  0.1035],
        [-0.0078, -0.0789,  0.0931,  ...,  0.0220, -0.0572,  0.1112]],
       requires_grad=True)
</pre>


<p>
This is truncated, so let's look at the shape instead to make sure it matches with our expectations.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">from</span> rich <span class="org-keyword">import</span> <span class="org-keyword">print</span>

<span class="org-keyword">print</span>(model.layers[0].weight.shape)
</pre>
</div>

<pre class="example">
torch.Size([30, 50])
</pre>


<p>
We can call on the model like this:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">X</span> = torch.rand((1,50))
<span class="org-variable-name">out</span> = model(X)
<span class="org-keyword">print</span>(out)
</pre>
</div>

<pre class="example">
tensor([[ 0.0623, -0.0063, -0.1485]], grad_fn=&lt;AddmmBackward0&gt;)
</pre>


<p>
We generated a single random example (50 dimensions) and passed it to the model. This was the <i>forward pass</i>. The forward pass simply means calculating the output tensors from the input tensors.
</p>

<p>
As we can see from the <code>grad_fn</code>, this forward pass computes a computational graph for backprop. This can be wasteful and unnecessary if we're just interested in inference. We use the <code>torch.no_grad</code> context manager to get around this.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">with</span> torch.no_grad():
    <span class="org-variable-name">out</span> = model(X)
<span class="org-keyword">print</span>(out)
</pre>
</div>

<pre class="example">
tensor([[ 0.0623, -0.0063, -0.1485]])
</pre>


<p>
And this approach just computes the output tensors.
</p>

<p>
Usually in PyTorch we don't pass the final layer to a nonlinear activation function, because the loss function usually combines softmax with negativel og-likelihood loss in a single class. We have to call softmax explicitly if we want class-membership probabilities.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">with</span> torch.no_grad():
    <span class="org-variable-name">out</span> = torch.softmax(model(X), dim=1)
<span class="org-keyword">print</span>(out)
</pre>
</div>

<pre class="example">
tensor([[0.3645, 0.3403, 0.2952]])
</pre>
</div>
</div>

<div id="outline-container-org5ff72ee" class="outline-2">
<h2 id="org5ff72ee">A.6 Setting up efficient data loaders</h2>
<div class="outline-text-2" id="text-org5ff72ee">
<p>
A <code>DataSet</code> is a class that defines how individual records are loaded. A <code>DataLoader</code> class handles dataset shuffling and assembling data records into batches.
</p>

<p>
This example shows a dataset of five training examples with two features each, along with a tensor of class labels. We also have a test dataset of two entries.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">X_train</span> = torch.tensor(
    [[-1.2, 3.1], [-0.9, 2.9], [-0.5, 2.6], [2.3, -1.1], [2.7, -1.5]]
)
<span class="org-variable-name">y_train</span> = torch.tensor([0, 0, 0, 1, 1])
<span class="org-variable-name">X_test</span> = torch.tensor(
    [
        [-0.8, 2.8],
        [2.6, -1.6],
    ]
)
<span class="org-variable-name">y_test</span> = torch.tensor([0, 1])
</pre>
</div>

<p>
Let's first make these into a <code>DataSet</code>.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">from</span> torch.utils.data <span class="org-keyword">import</span> Dataset

<span class="org-keyword">class</span> <span class="org-type">ToyDataset</span>(Dataset):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, X, y):
        <span class="org-keyword">self</span>.features = X
        <span class="org-keyword">self</span>.labels = y

    <span class="org-keyword">def</span> <span class="org-function-name">__getitem__</span>(<span class="org-keyword">self</span>, index):
        <span class="org-variable-name">one_x</span> = <span class="org-keyword">self</span>.features[index]
        <span class="org-variable-name">one_y</span> = <span class="org-keyword">self</span>.labels[index]
        <span class="org-keyword">return</span> one_x, one_y

    <span class="org-keyword">def</span> <span class="org-function-name">__len__</span>(<span class="org-keyword">self</span>):
        <span class="org-keyword">return</span> <span class="org-keyword">self</span>.labels.shape[0]

<span class="org-variable-name">train_ds</span> = ToyDataset(X_train, y_train)
<span class="org-variable-name">test_ds</span> = ToyDataset(X_test, y_test)
</pre>
</div>


<p>
Note the three main components of the above Dataset definition:
</p>
<ol class="org-ol">
<li><code>__init__</code>, to set up attributes we can access in the other methods. This might be file paths, file objects, database connectors, etc. Here we just use X and y, which we point toward the correct tensor objects in memory.</li>
<li><code>__getitem__</code> is for defining instructions for retrieving exactly one record via <code>index</code>.</li>
<li><p>
<code>__len__</code> is for retrieving the length of the dataset.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(<span class="org-builtin">len</span>(train_ds))
</pre>
</div>

<pre class="example">
5
</pre></li>
</ol>

<p>
Now we can use the <code>DataLoader</code> class to define how to sample from the Dataset we defined.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">from</span> torch.utils.data <span class="org-keyword">import</span> DataLoader

torch.manual_seed(123)

<span class="org-variable-name">train_loader</span> = DataLoader(
    dataset=train_ds,
    batch_size=2,
    shuffle=<span class="org-constant">True</span>,
    num_workers=0
    )

<span class="org-variable-name">test_loader</span> = DataLoader(
    dataset=test_ds,
    batch_size=2,
    shuffle=<span class="org-constant">False</span>,
    num_workers=0
    )
</pre>
</div>

<p>
Now we can iterate over the <code>train_loader</code> as follows:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">for</span> idx, (x, y) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(train_loader):
    <span class="org-keyword">print</span>(f<span class="org-string">"Batch {idx+1}:"</span>, x, y)
</pre>
</div>

<pre class="example">
Batch 1: tensor([[ 2.3000, -1.1000],
        [-0.9000,  2.9000]]) tensor([1, 0])

Batch 2: tensor([[-1.2000,  3.1000],
        [-0.5000,  2.6000]]) tensor([0, 0])

Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])
</pre>


<p>
Note that we can set <code>drop_last=True</code> to drop the last uneven batch, as significantly uneven batch sizes can harm convergence.
</p>

<p>
The <code>num_workers</code> argument relates to parallelizing data loading/processing. 0 indicates that it will all be done in the main process, not in separate worker processes. This can slow things down a lot.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2024-03-31 Sun 00:00</p>
<p class="creator"><a href="https://www.gnu.org/software/emacs/">Emacs</a> 27.1 (<a href="https://orgmode.org">Org</a> mode 9.3)</p>
</div>
</body>
</html>
