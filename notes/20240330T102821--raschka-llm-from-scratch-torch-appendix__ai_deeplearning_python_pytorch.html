<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>PyTorch Review</title>
<meta name="generator" content="Org mode">
<link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
<link rel="stylesheet" type="text/css" href="/orgstyle.css"/>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto+Slab&display=swap" rel="stylesheet">
<link rel="icon" href="favicon.ico" type="image/x-icon">
<link rel="me" href="https://emacs.ch/@dliden">
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2020 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="preamble" class="status">
<hr style="border-top: 1px solid black;">
<div class='topnav' style='display: flex; justify-content: space-between; align-items: center;'>
  <a href="/index.html"><h2 style='margin-top: 0; margin-bottom: 0; margin-left:0px;'>Daniel Liden</h2></a>
  <div>
    <a href='/archive.html' style='font-weight:bold; font-style:italic;'>Blog</a> / 
    <a href='/about.html' style='font-weight:bold; font-style:italic;'>About Me</a> /
    <a href='/photos.html' style='font-weight:bold; font-style:italic;'>Photos</a> /
    <a href='/fine-tuning/' style='font-weight:bold; font-style:italic;'>LLM Fine Tuning</a> /
    <a href='/notes.html' style='font-weight:bold; font-style:italic;'>Notes</a> /
    <a href='/rss.xml'>
      <img src='/rss.png' style='height: 1em;'>
    </a>
  </div>
</div>
<hr style="border-top: 1px solid black;">
</div>
<div id="content">
<header>
<h1 class="title">PyTorch Review</h1>
</header><nav id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org00be3aa">A.1 PyTorch</a></li>
<li><a href="#org3e6ec97">A.2 Understanding tensors</a>
<ul>
<li><a href="#org23e84e7">Creating tensors</a></li>
<li><a href="#orgebf0ab4">Tensor data types</a></li>
<li><a href="#orgb855cba">Common tensor operations</a></li>
</ul>
</li>
<li><a href="#org3b2183f">A.3 Models as Computational Graphs</a></li>
<li><a href="#org03668a9">A.4 Automatic Differentiation</a></li>
<li><a href="#org7076cd7">A.5 Implementing multilayer neural networks</a></li>
<li><a href="#orga89f559">A.6 Setting up efficient data loaders</a></li>
<li><a href="#orgbbda44f">A.7 A typical training loop</a></li>
</ul>
</div>
</nav>
<div class="preview">
<p>
This is a quick run through the appendix on PyTorch from Sebastian Raschka's <a href="https://www.manning.com/books/build-a-large-language-model-from-scratch">Build a Large Language Model (From Scratch)</a> book, currently available via Manning's MEAP. I haven't spent an enormous amount of time with PyTorch in the last year or so, so it seemed worth the effort to work through it.
</p>

</div>

<div id="outline-container-org00be3aa" class="outline-2">
<h2 id="org00be3aa">A.1 PyTorch</h2>
<div class="outline-text-2" id="text-org00be3aa">
<p>
There are three broad components to PyTorch:
</p>
<ul class="org-ul">
<li>A tensor library extending array-oriented programming from NumPy with additional features for accelerated computation on GPUs.</li>
<li>An automatic differentiation engine (autograd), which ehables automatic computation of gradients for tensor operations for backpropagation/model optimization.</li>
<li>A deep learning library, offering modular, flexible, and extensible building blocks for designing and training deep learning models.</li>
</ul>

<p>
Let's make sure we have it installed correctly&#x2026;
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> torch

torch.__version__
</pre>
</div>

<pre class="example">
2.2.2
</pre>


<p>
Let's make sure we can use <code>mps</code> (on mac).
</p>

<div class="org-src-container">
<pre class="src src-python">torch.backends.mps.is_available()
</pre>
</div>

<pre class="example">
True
</pre>


<p>
Great.
</p>
</div>
</div>
<div id="outline-container-org3e6ec97" class="outline-2">
<h2 id="org3e6ec97">A.2 Understanding tensors</h2>
<div class="outline-text-2" id="text-org3e6ec97">
<p>
Tensors generalize vectors and matrices to arbitrary dimensions. PyTorch tensors are similar to NumPy arrays but have have several additional features:
</p>
<ul class="org-ul">
<li>an automatic differentiation engine</li>
<li>gpu computation</li>
</ul>
<p>
Still, it has a numpy-like API.
</p>
</div>
<div id="outline-container-org23e84e7" class="outline-3">
<h3 id="org23e84e7">Creating tensors</h3>
<div class="outline-text-3" id="text-org23e84e7">
<div class="org-src-container">
<pre class="src src-python"><span class="org-comment-delimiter"># </span><span class="org-comment">0d tensor (scalar)</span>
<span class="org-keyword">print</span>(torch.tensor(1))

<span class="org-comment-delimiter"># </span><span class="org-comment">1d tensor (vector)</span>
<span class="org-keyword">print</span>(torch.tensor([1, 2, 3]))

<span class="org-comment-delimiter"># </span><span class="org-comment">2d tensor</span>
<span class="org-keyword">print</span>(torch.tensor([[1, 2], [3, 4]]))

<span class="org-comment-delimiter"># </span><span class="org-comment">3d tensor</span>
<span class="org-keyword">print</span>(torch.tensor([[[1, 2], [3,4]], [[5,6], [7,8]]]))
</pre>
</div>

<pre class="example">
tensor(1)
tensor([1, 2, 3])
tensor([[1, 2],
        [3, 4]])
tensor([[[1, 2],
         [3, 4]],

        [[5, 6],
         [7, 8]]])
</pre>
</div>
</div>
<div id="outline-container-orgebf0ab4" class="outline-3">
<h3 id="orgebf0ab4">Tensor data types</h3>
<div class="outline-text-3" id="text-orgebf0ab4">
<p>
These are important to pay attention to! So let's pay attention to them. The default (from above) is the 64-bit integer.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">tensor1d</span> = torch.tensor([1,2,3])
<span class="org-keyword">print</span>(tensor1d.dtype)
</pre>
</div>

<pre class="example">
torch.int64
</pre>


<p>
For floats, PyTorch uses 32-bit precision by default.
</p>


<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">floatvec</span> = torch.tensor([1., 2., 3.])
<span class="org-keyword">print</span>(floatvec.dtype)
</pre>
</div>

<pre class="example">
torch.float32
</pre>


<p>
Why this default?
</p>
<ul class="org-ul">
<li>GPU architectures are optimized for 32-bit computations</li>
<li>32-bit precision is sufficient for most deep learning tasks but uses less memory and computational resources than 64-bit.</li>
</ul>

<p>
it is easy to change <code>dtype</code> (and precision) with a tensor's <code>.to</code> method.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(torch.tensor([1,2,3]).to(torch.float32).dtype)
</pre>
</div>

<pre class="example">
torch.float32
</pre>
</div>
</div>
<div id="outline-container-orgb855cba" class="outline-3">
<h3 id="orgb855cba">Common tensor operations</h3>
<div class="outline-text-3" id="text-orgb855cba">
<ul class="org-ul">
<li>brief survey of the most common tensor operations prior to getting into the computational graphs concept.</li>
</ul>


<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">tensor2d</span> = torch.tensor([[1, 2, 3], [4, 5, 6]])
</pre>
</div>

<p>
Reshape:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(tensor2d.reshape(3, 2))
</pre>
</div>

<pre class="example">
tensor([[1, 2],
        [3, 4],
        [5, 6]])
</pre>


<p>
It is more common to use <code>view</code> than <code>reshape</code>.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(tensor2d.view(3, 2))
</pre>
</div>

<pre class="example">
tensor([[1, 2],
        [3, 4],
        [5, 6]])
</pre>


<p>
Transpose
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(tensor2d.T)
</pre>
</div>

<pre class="example">
tensor([[1, 4],
        [2, 5],
        [3, 6]])
</pre>



<p>
Matrix multiplication is usually handled with <code>matmul</code>.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(tensor2d.matmul(tensor2d.T))
</pre>
</div>

<pre class="example">
tensor([[14, 32],
        [32, 77]])
</pre>


<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(tensor2d @ tensor2d.T)
</pre>
</div>

<pre class="example">
tensor([[14, 32],
        [32, 77]])
</pre>
</div>
</div>
</div>
<div id="outline-container-org3b2183f" class="outline-2">
<h2 id="org3b2183f">A.3 Models as Computational Graphs</h2>
<div class="outline-text-2" id="text-org3b2183f">
<p>
The previous section covered PyTorch's tensor library. This section gets into its automatic differentiation engine (autograd). Autograd provides functions for automatically computing gradients in dynamic computational graphs.
</p>

<p>
So what's a computational graph? It lays out the sequence of calculations needed to compute the gradients for backprop. We'll go through an example showing the forward pass of a logstic regression classifier.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> torch.nn.functional <span class="org-keyword">as</span> F

<span class="org-variable-name">y</span> = torch.tensor([1.0])
<span class="org-variable-name">x1</span> = torch.tensor([1.1])
<span class="org-variable-name">w1</span> = torch.tensor([2.2])
<span class="org-variable-name">b</span> = torch.tensor([0.0])

<span class="org-variable-name">z</span> = x1 * w1 + b
<span class="org-variable-name">a</span> = torch.sigmoid(z)

<span class="org-variable-name">loss</span> = F.binary_cross_entropy(a,y)
</pre>
</div>

<p>
This results in a computational graph which PyTorch builds in the background.
</p>

<p>
Input and weight -&gt; (u = w<sub>1</sub> * x<sub>1</sub>) -&gt; +b -&gt; (z = u + b) -&gt; (a = &sigma;(z)) -&gt; loss = L(a,y) &lt;- y
</p>
</div>
</div>
<div id="outline-container-org03668a9" class="outline-2">
<h2 id="org03668a9">A.4 Automatic Differentiation</h2>
<div class="outline-text-2" id="text-org03668a9">
<p>
PyTorch will automatically build such a graph if one of its terminal nodes has the <code>requires_grad</code> attribute set to True. This enables us to train neural nets via backpropagation. Working backward from the above:
</p>

\begin{align*}
\frac{\partial L}{\partial w_1} &= \frac{\partial u}{\partial w_1} \times \frac{\partial z}{\partial u} \times \frac{\partial a}{\partial z} \times \frac{\partial L}{\partial a} \\
\frac{\partial L}{\partial b} &= \frac{\partial z}{\partial b} \times \frac{\partial a}{\partial z} \times \frac{\partial L}{\partial a}
\end{align*}

<p>
Basically&#x2013;apply the chain rule right to left.
</p>

<p>
Quick reminder of some definitions:
</p>
<ul class="org-ul">
<li>a partial derivative measures the rate at which a function changes w/r/t one of its variables</li>
<li>a gradient is a vector of all the partial derivatives of a multivariate function</li>
</ul>

<p>
So what exactly does this have to do with torch as an autograd engine? PyTorch tracks every operation performed on tensors and can, therefore, construct a computational graph in the background. Then it cal cann on the <code>grad</code> function to compute the gradient of the loss w/r/t the model parameter as follows:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> torch.nn.functional <span class="org-keyword">as</span> F
<span class="org-keyword">from</span> torch.autograd <span class="org-keyword">import</span> grad

<span class="org-variable-name">y</span> = torch.tensor([1.0])
<span class="org-variable-name">x1</span> = torch.tensor([1.1])
<span class="org-variable-name">w1</span> = torch.tensor([2.2], requires_grad=<span class="org-constant">True</span>)
<span class="org-variable-name">b</span> = torch.tensor([0.0], requires_grad=<span class="org-constant">True</span>)

<span class="org-variable-name">z</span> = x1 * w1 + b
<span class="org-variable-name">a</span> = torch.sigmoid(z)

<span class="org-variable-name">loss</span> = F.binary_cross_entropy(a, y)
<span class="org-variable-name">grad_L_w1</span> = grad(loss, w1, retain_graph=<span class="org-constant">True</span>) <span class="org-comment-delimiter">#</span><span class="org-comment">A</span>
<span class="org-variable-name">grad_L_b</span> = grad(loss, b, retain_graph=<span class="org-constant">True</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(grad_L_w1)
<span class="org-keyword">print</span>(grad_L_b)
</pre>
</div>

<pre class="example">
(tensor([-0.0898]),)
(tensor([-0.0817]),)
</pre>


<p>
We seldom manually call the grad function. We usually call <code>.backward</code> on the loss, which computes the gradients of all the leaf nodes in the graph, which will be stored via the <code>.grad</code> attributes of the tensors.
</p>


<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(loss.backward())
<span class="org-keyword">print</span>(w1.grad)
<span class="org-keyword">print</span>(b.grad)
</pre>
</div>

<pre class="example">
None
tensor([-0.0898])
tensor([-0.0817])
</pre>
</div>
</div>
<div id="outline-container-org7076cd7" class="outline-2">
<h2 id="org7076cd7">A.5 Implementing multilayer neural networks</h2>
<div class="outline-text-2" id="text-org7076cd7">
<p>
Now we get to the third major component of Pytorch: its library for implementing deep neural networks.
</p>

<p>
We will focus on a fully-connected MLP. To implement an NN in PyTorch, we:
</p>
<ul class="org-ul">
<li>subclass the <code>torch.nn.Module</code> class to define a custom architecture</li>
<li>define layers within the <code>__init__</code> constructor of the module subclass, specifying how they interact in the forward method.</li>
<li>defined the forward method, which describes how data passes through the network and relates as a computational graph.</li>
</ul>

<p>
We generally do not need to implement the <code>backward</code> method ourselves.
</p>

<p>
Here is code illustrating a basic NN with two hidden layers.
</p>


<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">class</span> <span class="org-type">NeuralNetwork</span>(torch.nn.Module):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, num_inputs, num_outputs):
        <span class="org-builtin">super</span>().__init__()

        <span class="org-keyword">self</span>.layers = torch.nn.Sequential(
            <span class="org-comment-delimiter"># </span><span class="org-comment">1st hidden layer</span>
            torch.nn.Linear(num_inputs, 30),
            torch.nn.ReLU(),
            <span class="org-comment-delimiter"># </span><span class="org-comment">2nd hidden layer</span>
            torch.nn.Linear(30, 20),
            torch.nn.ReLU(),
            <span class="org-comment-delimiter"># </span><span class="org-comment">output layer</span>
            torch.nn.Linear(20, num_outputs),
        )

    <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x):
        <span class="org-variable-name">logits</span> = <span class="org-keyword">self</span>.layers(x)
        <span class="org-keyword">return</span> logits
</pre>
</div>

<p>
We can instantiate this with 50 inputs and 3 outputs.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">model</span> = NeuralNetwork(50, 3)
<span class="org-keyword">print</span>(model)
</pre>
</div>

<pre class="example">
NeuralNetwork(
  (layers): Sequential(
    (0): Linear(in_features=50, out_features=30, bias=True)
    (1): ReLU()
    (2): Linear(in_features=30, out_features=20, bias=True)
    (3): ReLU()
    (4): Linear(in_features=20, out_features=3, bias=True)
  )
)
</pre>


<p>
We can count the total number of trainable parameters as follows:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">num_params</span> = <span class="org-builtin">sum</span>(p.numel() <span class="org-keyword">for</span> p <span class="org-keyword">in</span> model.parameters() <span class="org-keyword">if</span> p.requires_grad)
<span class="org-keyword">print</span>(<span class="org-string">"Total number of trainable model parameters:"</span>, num_params)
</pre>
</div>

<pre class="example">
Total number of trainable model parameters: 2213
</pre>


<p>
A parameter is <i>trainable</i> if its <code>requires_grad</code> attribute is <code>True</code>. We can investigate specific layers. Let's look at the first linear layer.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(model.layers[0].weight)
</pre>
</div>

<pre class="example">
Parameter containing:
tensor([[-0.0844,  0.0863,  0.1168,  ...,  0.0203, -0.0814, -0.0504],
        [ 0.0288,  0.0004, -0.1411,  ..., -0.0322, -0.1085,  0.0682],
        [-0.1075, -0.0173, -0.0476,  ..., -0.0684, -0.0522, -0.1316],
        ...,
        [ 0.1129, -0.0639, -0.0662,  ...,  0.1284, -0.0707,  0.1090],
        [ 0.0790, -0.1206, -0.1156,  ...,  0.1393, -0.0233,  0.1035],
        [-0.0078, -0.0789,  0.0931,  ...,  0.0220, -0.0572,  0.1112]],
       requires_grad=True)
</pre>


<p>
This is truncated, so let's look at the shape instead to make sure it matches with our expectations.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">from</span> rich <span class="org-keyword">import</span> <span class="org-keyword">print</span>

<span class="org-keyword">print</span>(model.layers[0].weight.shape)
</pre>
</div>

<pre class="example">
torch.Size([30, 50])
</pre>


<p>
We can call on the model like this:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">X</span> = torch.rand((1,50))
<span class="org-variable-name">out</span> = model(X)
<span class="org-keyword">print</span>(out)
</pre>
</div>

<pre class="example">
tensor([[ 0.0623, -0.0063, -0.1485]], grad_fn=&lt;AddmmBackward0&gt;)
</pre>


<p>
We generated a single random example (50 dimensions) and passed it to the model. This was the <i>forward pass</i>. The forward pass simply means calculating the output tensors from the input tensors.
</p>

<p>
As we can see from the <code>grad_fn</code>, this forward pass computes a computational graph for backprop. This can be wasteful and unnecessary if we're just interested in inference. We use the <code>torch.no_grad</code> context manager to get around this.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">with</span> torch.no_grad():
    <span class="org-variable-name">out</span> = model(X)
<span class="org-keyword">print</span>(out)
</pre>
</div>

<pre class="example">
tensor([[ 0.0623, -0.0063, -0.1485]])
</pre>


<p>
And this approach just computes the output tensors.
</p>

<p>
Usually in PyTorch we don't pass the final layer to a nonlinear activation function, because the loss function usually combines softmax with negativel og-likelihood loss in a single class. We have to call softmax explicitly if we want class-membership probabilities.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">with</span> torch.no_grad():
    <span class="org-variable-name">out</span> = torch.softmax(model(X), dim=1)
<span class="org-keyword">print</span>(out)
</pre>
</div>

<pre class="example">
tensor([[0.3645, 0.3403, 0.2952]])
</pre>
</div>
</div>

<div id="outline-container-orga89f559" class="outline-2">
<h2 id="orga89f559">A.6 Setting up efficient data loaders</h2>
<div class="outline-text-2" id="text-orga89f559">
<p>
A <code>DataSet</code> is a class that defines how individual records are loaded. A <code>DataLoader</code> class handles dataset shuffling and assembling data records into batches.
</p>

<p>
This example shows a dataset of five training examples with two features each, along with a tensor of class labels. We also have a test dataset of two entries.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">X_train</span> = torch.tensor(
    [[-1.2, 3.1], [-0.9, 2.9], [-0.5, 2.6], [2.3, -1.1], [2.7, -1.5]]
)
<span class="org-variable-name">y_train</span> = torch.tensor([0, 0, 0, 1, 1])
<span class="org-variable-name">X_test</span> = torch.tensor(
    [
        [-0.8, 2.8],
        [2.6, -1.6],
    ]
)
<span class="org-variable-name">y_test</span> = torch.tensor([0, 1])
</pre>
</div>

<p>
Let's first make these into a <code>DataSet</code>.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">from</span> torch.utils.data <span class="org-keyword">import</span> Dataset

<span class="org-keyword">class</span> <span class="org-type">ToyDataset</span>(Dataset):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, X, y):
        <span class="org-keyword">self</span>.features = X
        <span class="org-keyword">self</span>.labels = y

    <span class="org-keyword">def</span> <span class="org-function-name">__getitem__</span>(<span class="org-keyword">self</span>, index):
        <span class="org-variable-name">one_x</span> = <span class="org-keyword">self</span>.features[index]
        <span class="org-variable-name">one_y</span> = <span class="org-keyword">self</span>.labels[index]
        <span class="org-keyword">return</span> one_x, one_y

    <span class="org-keyword">def</span> <span class="org-function-name">__len__</span>(<span class="org-keyword">self</span>):
        <span class="org-keyword">return</span> <span class="org-keyword">self</span>.labels.shape[0]

<span class="org-variable-name">train_ds</span> = ToyDataset(X_train, y_train)
<span class="org-variable-name">test_ds</span> = ToyDataset(X_test, y_test)
</pre>
</div>


<p>
Note the three main components of the above Dataset definition:
</p>
<ol class="org-ol">
<li><code>__init__</code>, to set up attributes we can access in the other methods. This might be file paths, file objects, database connectors, etc. Here we just use X and y, which we point toward the correct tensor objects in memory.</li>
<li><code>__getitem__</code> is for defining instructions for retrieving exactly one record via <code>index</code>.</li>
<li><p>
<code>__len__</code> is for retrieving the length of the dataset.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(<span class="org-builtin">len</span>(train_ds))
</pre>
</div>

<pre class="example">
5
</pre></li>
</ol>

<p>
Now we can use the <code>DataLoader</code> class to define how to sample from the Dataset we defined.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">from</span> torch.utils.data <span class="org-keyword">import</span> DataLoader

torch.manual_seed(123)

<span class="org-variable-name">train_loader</span> = DataLoader(
    dataset=train_ds,
    batch_size=2,
    shuffle=<span class="org-constant">True</span>,
    num_workers=0
    )

<span class="org-variable-name">test_loader</span> = DataLoader(
    dataset=test_ds,
    batch_size=2,
    shuffle=<span class="org-constant">False</span>,
    num_workers=0
    )
</pre>
</div>

<p>
Now we can iterate over the <code>train_loader</code> as follows:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">for</span> idx, (x, y) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(train_loader):
    <span class="org-keyword">print</span>(f<span class="org-string">"Batch {idx+1}:"</span>, x, y)
</pre>
</div>

<pre class="example">
Batch 1: tensor([[ 2.3000, -1.1000],
        [-0.9000,  2.9000]]) tensor([1, 0])

Batch 2: tensor([[-1.2000,  3.1000],
        [-0.5000,  2.6000]]) tensor([0, 0])

Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])
</pre>


<p>
Note that we can set <code>drop_last=True</code> to drop the last uneven batch, as significantly uneven batch sizes can harm convergence.
</p>

<p>
The <code>num_workers</code> argument relates to parallelizing data loading/processing. 0 indicates that it will all be done in the main process, not in separate worker processes. This can slow things down a lot.
</p>
</div>
</div>

<div id="outline-container-orgbbda44f" class="outline-2">
<h2 id="orgbbda44f">A.7 A typical training loop</h2>
<div class="outline-text-2" id="text-orgbbda44f">
<p>
In this section, we combine many of the techniques from above to show a complete training loop.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> torch
<span class="org-keyword">import</span> torch.nn.functional <span class="org-keyword">as</span> F
 
 
torch.manual_seed(123)
<span class="org-variable-name">model</span> = NeuralNetwork(num_inputs=2, num_outputs=2)
<span class="org-variable-name">optimizer</span> = torch.optim.SGD(model.parameters(), lr=0.5)
 
<span class="org-variable-name">num_epochs</span> = 3
 
<span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(num_epochs): 
    
    model.train()
    <span class="org-keyword">for</span> batch_idx, (features, labels) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(train_loader):
 
        <span class="org-variable-name">logits</span> = model(features)
        
        <span class="org-variable-name">loss</span> = F.cross_entropy(logits, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
        <span class="org-comment-delimiter">### </span><span class="org-comment">LOGGING</span>
        <span class="org-keyword">print</span>(f<span class="org-string">"Epoch: {epoch+1:03d}/{num_epochs:03d}"</span>
              f<span class="org-string">" | Batch {batch_idx:03d}/{len(train_loader):03d}"</span>
              f<span class="org-string">" | Train Loss: {loss:.2f}"</span>)
 
    model.<span class="org-builtin">eval</span>()
    <span class="org-comment-delimiter"># </span><span class="org-comment">Optional model evaluation</span>
</pre>
</div>

<pre class="example">
Epoch: 001/003 | Batch 000/003 | Train Loss: 0.75
Epoch: 001/003 | Batch 001/003 | Train Loss: 0.65
Epoch: 001/003 | Batch 002/003 | Train Loss: 0.42
Epoch: 002/003 | Batch 000/003 | Train Loss: 0.05
Epoch: 002/003 | Batch 001/003 | Train Loss: 0.13
Epoch: 002/003 | Batch 002/003 | Train Loss: 0.00
Epoch: 003/003 | Batch 000/003 | Train Loss: 0.01
Epoch: 003/003 | Batch 001/003 | Train Loss: 0.00
Epoch: 003/003 | Batch 002/003 | Train Loss: 0.02
</pre>



<p>
Note the use of <code>model.train</code> and <code>model.eval</code>. These set the model into training and evaluation mode, respectively. Some components behave differently during training or inference, such as ddropout or batch normalization. We don't have these or anything like them, so this is redundant in our code, but still good practice.
</p>

<p>
We pass the logits directly to <code>cross_entropy</code> to compute the loss and call <code>loss.backward()</code> to compute gradients. <code>optimizer.step</code> uses the gradients to update the model parameters.
</p>

<p>
It is important that we include an <code>optimizer.zero_grad</code> call in each update to reset the gradients and ensure they do not accumulate.
</p>

<p>
Now we can make predictions with the model.
</p>

<div class="org-src-container">
<pre class="src src-python">model.<span class="org-builtin">eval</span>()
<span class="org-keyword">with</span> torch.no_grad():
    <span class="org-variable-name">outputs</span> = model(X_train)
<span class="org-keyword">print</span>(outputs)
</pre>
</div>

<pre class="example">
tensor([[ 2.9320, -4.2563],
        [ 2.6045, -3.8389],
        [ 2.1484, -3.2514],
        [-2.1461,  2.1496],
        [-2.5004,  2.5210]])
</pre>


<p>
If we want the class membership, we can obtain it with:
</p>

<div class="org-src-container">
<pre class="src src-python">torch.set_printoptions(sci_mode=<span class="org-constant">False</span>)
<span class="org-variable-name">probas</span> = torch.softmax(outputs, dim=1)
<span class="org-keyword">print</span>(probas)
</pre>
</div>

<pre class="example">
tensor([[    0.9992,     0.0008],
        [    0.9984,     0.0016],
        [    0.9955,     0.0045],
        [    0.0134,     0.9866],
        [    0.0066,     0.9934]])
</pre>


<p>
There are two classes, so the above represents the probabilities of belonging to class 1 or class 2. The first three have high probability of class 1; the last two of class 2.
</p>

<p>
We can convery into class labels as follows:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">predictions</span> = torch.argmax(probas, dim=1)
<span class="org-keyword">print</span>(predictions)
</pre>
</div>

<pre class="example">
tensor([0, 0, 0, 1, 1])
</pre>


<p>
We don't need to compute softmax probabilities to accomplish this.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(torch.argmax(outputs, dim=1))
</pre>
</div>

<pre class="example">
tensor([0, 0, 0, 1, 1])
</pre>


<p>
Is it correct?
</p>

<div class="org-src-container">
<pre class="src src-python">predictions == y_train
</pre>
</div>

<pre class="example">
tensor([True, True, True, True, True])
</pre>


<p>
and to get the proportion correct:
</p>
<div class="org-src-container">
<pre class="src src-python">torch.<span class="org-builtin">sum</span>(predictions == y_train) / <span class="org-builtin">len</span>(y_train)
</pre>
</div>

<pre class="example">
tensor(1.)
</pre>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2024-03-31 Sun 00:00</p>
<p class="creator"><a href="https://www.gnu.org/software/emacs/">Emacs</a> 27.1 (<a href="https://orgmode.org">Org</a> mode 9.3)</p>
</div>
</body>
</html>
