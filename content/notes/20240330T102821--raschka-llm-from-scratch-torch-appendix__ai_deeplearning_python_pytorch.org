#+title: PyTorch Review
#+date:      <2024-03-31 Sun>
#+filetags:   :ai:deeplearning:python:pytorch:
#+identifier: 20240330T102821

#+begin_preview
This is a quick run through the appendix on PyTorch from Sebastian Raschka's [[https://www.manning.com/books/build-a-large-language-model-from-scratch][Build a Large Language Model (From Scratch)]] book, currently available via Manning's MEAP. I haven't spent an enormous amount of time with PyTorch in the last year or so, so it seemed worth the effort to work through it.
#+end_preview

* A.1 PyTorch
There are three broad components to PyTorch:
- A tensor library extending array-oriented programming from NumPy with additional features for accelerated computation on GPUs.
- An automatic differentiation engine (autograd), which ehables automatic computation of gradients for tensor operations for backpropagation/model optimization.
- A deep learning library, offering modular, flexible, and extensible building blocks for designing and training deep learning models.

Let's make sure we have it installed correctly...

#+begin_src python :session appA
import torch

torch.__version__
#+end_src

#+RESULTS:
: 2.2.2

Let's make sure we can use ~mps~ (on mac).

#+begin_src python :session appA
torch.backends.mps.is_available()
#+end_src

#+RESULTS:
: True

Great.
* A.2 Understanding tensors

Tensors generalize vectors and matrices to arbitrary dimensions. PyTorch tensors are similar to NumPy arrays but have have several additional features:
- an automatic differentiation engine
- gpu computation
Still, it has a numpy-like API.
** Creating tensors

#+begin_src python :session appA :results output
# 0d tensor (scalar)
print(torch.tensor(1))

# 1d tensor (vector)
print(torch.tensor([1, 2, 3]))

# 2d tensor
print(torch.tensor([[1, 2], [3, 4]]))

# 3d tensor
print(torch.tensor([[[1, 2], [3,4]], [[5,6], [7,8]]]))
#+end_src

#+RESULTS:
: tensor(1)
: tensor([1, 2, 3])
: tensor([[1, 2],
:         [3, 4]])
: tensor([[[1, 2],
:          [3, 4]],
: 
:         [[5, 6],
:          [7, 8]]])
** Tensor data types
These are important to pay attention to! So let's pay attention to them. The default (from above) is the 64-bit integer.

#+begin_src python :session appA :results output
tensor1d = torch.tensor([1,2,3])
print(tensor1d.dtype)
#+end_src

#+RESULTS:
: torch.int64

For floats, PyTorch uses 32-bit precision by default.


#+begin_src python :session appA :results output
floatvec = torch.tensor([1., 2., 3.])
print(floatvec.dtype)
#+end_src

#+RESULTS:
: torch.float32

Why this default?
- GPU architectures are optimized for 32-bit computations
- 32-bit precision is sufficient for most deep learning tasks but uses less memory and computational resources than 64-bit.

it is easy to change ~dtype~ (and precision) with a tensor's ~.to~ method.

#+begin_src python :session appA :results output
print(torch.tensor([1,2,3]).to(torch.float32).dtype)
#+end_src

#+RESULTS:
: torch.float32
** Common tensor operations
- brief survey of the most common tensor operations prior to getting into the computational graphs concept.


#+begin_src python :session appA :results output
tensor2d = torch.tensor([[1, 2, 3], [4, 5, 6]])
#+end_src

#+RESULTS:

Reshape:

#+begin_src python :session appA :results output
print(tensor2d.reshape(3, 2))
#+end_src

#+RESULTS:
: tensor([[1, 2],
:         [3, 4],
:         [5, 6]])

It is more common to use ~view~ than ~reshape~.

#+begin_src python :session appA :results output
print(tensor2d.view(3, 2))
#+end_src

#+RESULTS:
: tensor([[1, 2],
:         [3, 4],
:         [5, 6]])

Transpose

#+begin_src python :session appA :results output
print(tensor2d.T)
#+end_src

#+RESULTS:
: tensor([[1, 4],
:         [2, 5],
:         [3, 6]])


Matrix multiplication is usually handled with ~matmul~.

#+begin_src python :session appA :results output
print(tensor2d.matmul(tensor2d.T))
#+end_src

#+RESULTS:
: tensor([[14, 32],
:         [32, 77]])

#+begin_src python :session appA :results output
print(tensor2d @ tensor2d.T)
#+end_src

#+RESULTS:
: tensor([[14, 32],
:         [32, 77]])
* A.3 Models as Computational Graphs

The previous section covered PyTorch's tensor library. This section gets into its automatic differentiation engine (autograd). Autograd provides functions for automatically computing gradients in dynamic computational graphs.

So what's a computational graph? It lays out the sequence of calculations needed to compute the gradients for backprop. We'll go through an example showing the forward pass of a logstic regression classifier.

#+begin_src python :session appA :results output
import torch.nn.functional as F

y = torch.tensor([1.0])
x1 = torch.tensor([1.1])
w1 = torch.tensor([2.2])
b = torch.tensor([0.0])

z = x1 * w1 + b
a = torch.sigmoid(z)

loss = F.binary_cross_entropy(a,y)
#+end_src

#+RESULTS:


This results in a computational graph which PyTorch builds in the background.

Input and weight -> (u = w_1 * x_1) -> +b -> (z = u + b) -> (a = \sigma(z)) -> loss = L(a,y) <- y
* A.4 Automatic Differentiation
PyTorch will automatically build such a graph if one of its terminal nodes has the ~requires_grad~ attribute set to True. This enables us to train neural nets via backpropagation. Working backward from the above:

\begin{align*}
\frac{\partial L}{\partial w_1} &= \frac{\partial u}{\partial w_1} \times \frac{\partial z}{\partial u} \times \frac{\partial a}{\partial z} \times \frac{\partial L}{\partial a} \\
\frac{\partial L}{\partial b} &= \frac{\partial z}{\partial b} \times \frac{\partial a}{\partial z} \times \frac{\partial L}{\partial a}
\end{align*}

Basically--apply the chain rule right to left.

Quick reminder of some definitions:
- a partial derivative measures the rate at which a function changes w/r/t one of its variables 
- a gradient is a vector of all the partial derivatives of a multivariate function

So what exactly does this have to do with torch as an autograd engine? PyTorch tracks every operation performed on tensors and can, therefore, construct a computational graph in the background. Then it cal cann on the ~grad~ function to compute the gradient of the loss w/r/t the model parameter as follows:

#+begin_src python :session appA :results output
import torch.nn.functional as F
from torch.autograd import grad

y = torch.tensor([1.0])
x1 = torch.tensor([1.1])
w1 = torch.tensor([2.2], requires_grad=True)
b = torch.tensor([0.0], requires_grad=True)

z = x1 * w1 + b
a = torch.sigmoid(z)

loss = F.binary_cross_entropy(a, y)
grad_L_w1 = grad(loss, w1, retain_graph=True) #A
grad_L_b = grad(loss, b, retain_graph=True)
#+end_src

#+RESULTS:

#+begin_src python :session appA :results output
print(grad_L_w1)
print(grad_L_b)
#+end_src

#+RESULTS:
: (tensor([-0.0898]),)
: (tensor([-0.0817]),)

We seldom manually call the grad function. We usually call ~.backward~ on the loss, which computes the gradients of all the leaf nodes in the graph, which will be stored via the ~.grad~ attributes of the tensors.


#+begin_src python :session appA :results output
print(loss.backward())
print(w1.grad)
print(b.grad)
#+end_src

#+RESULTS:
: None
: tensor([-0.0898])
: tensor([-0.0817])
* A.5 Implementing multilayer neural networks
Now we get to the third major component of Pytorch: its library for implementing deep neural networks.

We will focus on a fully-connected MLP. To implement an NN in PyTorch, we:
- subclass the ~torch.nn.Module~ class to define a custom architecture
- define layers within the ~__init__~ constructor of the module subclass, specifying how they interact in the forward method.
- defined the forward method, which describes how data passes through the network and relates as a computational graph.

We generally do not need to implement the ~backward~ method ourselves.

Here is code illustrating a basic NN with two hidden layers.


#+begin_src python :session appA :results output
class NeuralNetwork(torch.nn.Module):
    def __init__(self, num_inputs, num_outputs):
        super().__init__()

        self.layers = torch.nn.Sequential(
            # 1st hidden layer
            torch.nn.Linear(num_inputs, 30),
            torch.nn.ReLU(),
            # 2nd hidden layer
            torch.nn.Linear(30, 20),
            torch.nn.ReLU(),
            # output layer
            torch.nn.Linear(20, num_outputs),
        )

    def forward(self, x):
        logits = self.layers(x)
        return logits
#+end_src

#+RESULTS:


We can instantiate this with 50 inputs and 3 outputs.

#+begin_src python :session appA :results output
model = NeuralNetwork(50, 3)
print(model)
#+end_src

#+RESULTS:
: NeuralNetwork(
:   (layers): Sequential(
:     (0): Linear(in_features=50, out_features=30, bias=True)
:     (1): ReLU()
:     (2): Linear(in_features=30, out_features=20, bias=True)
:     (3): ReLU()
:     (4): Linear(in_features=20, out_features=3, bias=True)
:   )
: )

We can count the total number of trainable parameters as follows:

#+begin_src python :session appA :results output
num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print("Total number of trainable model parameters:", num_params)
#+end_src

#+RESULTS:
: Total number of trainable model parameters: 2213

A parameter is /trainable/ if its ~requires_grad~ attribute is ~True~. We can investigate specific layers. Let's look at the first linear layer.

#+begin_src python :session appA :results output
print(model.layers[0].weight)
#+end_src

#+RESULTS:
: Parameter containing:
: tensor([[-0.0844,  0.0863,  0.1168,  ...,  0.0203, -0.0814, -0.0504],
:         [ 0.0288,  0.0004, -0.1411,  ..., -0.0322, -0.1085,  0.0682],
:         [-0.1075, -0.0173, -0.0476,  ..., -0.0684, -0.0522, -0.1316],
:         ...,
:         [ 0.1129, -0.0639, -0.0662,  ...,  0.1284, -0.0707,  0.1090],
:         [ 0.0790, -0.1206, -0.1156,  ...,  0.1393, -0.0233,  0.1035],
:         [-0.0078, -0.0789,  0.0931,  ...,  0.0220, -0.0572,  0.1112]],
:        requires_grad=True)

This is truncated, so let's look at the shape instead to make sure it matches with our expectations.

#+begin_src python :session appA :results output
from rich import print

print(model.layers[0].weight.shape)
#+end_src

#+RESULTS:
: torch.Size([30, 50])

We can call on the model like this:

#+begin_src python :session appA :results output
X = torch.rand((1,50))
out = model(X)
print(out)
#+end_src

#+RESULTS:
: tensor([[ 0.0623, -0.0063, -0.1485]], grad_fn=<AddmmBackward0>)

We generated a single random example (50 dimensions) and passed it to the model. This was the /forward pass/. The forward pass simply means calculating the output tensors from the input tensors.

As we can see from the ~grad_fn~, this forward pass computes a computational graph for backprop. This can be wasteful and unnecessary if we're just interested in inference. We use the ~torch.no_grad~ context manager to get around this.

#+begin_src python :session appA :results output
with torch.no_grad():
    out = model(X)
print(out)
#+end_src

#+RESULTS:
: tensor([[ 0.0623, -0.0063, -0.1485]])

And this approach just computes the output tensors.

Usually in PyTorch we don't pass the final layer to a nonlinear activation function, because the loss function usually combines softmax with negativel og-likelihood loss in a single class. We have to call softmax explicitly if we want class-membership probabilities.

#+begin_src python :session appA :results output
with torch.no_grad():
    out = torch.softmax(model(X), dim=1)
print(out)
#+end_src

#+RESULTS:
: tensor([[0.3645, 0.3403, 0.2952]])

* A.6 Setting up efficient data loaders
A ~DataSet~ is a class that defines how individual records are loaded. A ~DataLoader~ class handles dataset shuffling and assembling data records into batches.

This example shows a dataset of five training examples with two features each, along with a tensor of class labels. We also have a test dataset of two entries.

#+begin_src python :session appA :results output
X_train = torch.tensor(
    [[-1.2, 3.1], [-0.9, 2.9], [-0.5, 2.6], [2.3, -1.1], [2.7, -1.5]]
)
y_train = torch.tensor([0, 0, 0, 1, 1])
X_test = torch.tensor(
    [
        [-0.8, 2.8],
        [2.6, -1.6],
    ]
)
y_test = torch.tensor([0, 1])
#+end_src

#+RESULTS:

Let's first make these into a ~DataSet~.

#+begin_src python :session appA :results output
from torch.utils.data import Dataset

class ToyDataset(Dataset):
    def __init__(self, X, y):
        self.features = X
        self.labels = y

    def __getitem__(self, index):
        one_x = self.features[index]
        one_y = self.labels[index]
        return one_x, one_y

    def __len__(self):
        return self.labels.shape[0]

train_ds = ToyDataset(X_train, y_train)
test_ds = ToyDataset(X_test, y_test)
#+end_src


Note the three main components of the above Dataset definition:
1. ~__init__~, to set up attributes we can access in the other methods. This might be file paths, file objects, database connectors, etc. Here we just use X and y, which we point toward the correct tensor objects in memory.
2. ~__getitem__~ is for defining instructions for retrieving exactly one record via ~index~.
3. ~__len__~ is for retrieving the length of the dataset.

   #+begin_src python :session appA :results output
print(len(train_ds))
   #+end_src

   #+RESULTS:
   : 5
   
Now we can use the ~DataLoader~ class to define how to sample from the Dataset we defined.

#+begin_src python :session appA :results output
from torch.utils.data import DataLoader

torch.manual_seed(123)

train_loader = DataLoader(
    dataset=train_ds,
    batch_size=2,
    shuffle=True,
    num_workers=0
    )

test_loader = DataLoader(
    dataset=test_ds,
    batch_size=2,
    shuffle=False,
    num_workers=0
    )
#+end_src

#+RESULTS:

Now we can iterate over the ~train_loader~ as follows:

#+begin_src python :session appA :results output
for idx, (x, y) in enumerate(train_loader):
    print(f"Batch {idx+1}:", x, y)
#+end_src

#+RESULTS:
: Batch 1: tensor([[ 2.3000, -1.1000],
:         [-0.9000,  2.9000]]) tensor([1, 0])
: 
: Batch 2: tensor([[-1.2000,  3.1000],
:         [-0.5000,  2.6000]]) tensor([0, 0])
: 
: Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])

Note that we can set ~drop_last=True~ to drop the last uneven batch, as significantly uneven batch sizes can harm convergence.

The ~num_workers~ argument relates to parallelizing data loading/processing. 0 indicates that it will all be done in the main process, not in separate worker processes. This can slow things down a lot.
